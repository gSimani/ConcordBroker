# Supabase-Optimized Bulk Loading Implementation

## ðŸŽ‰ Supabase Response: 10/10 - EXCELLENT!

Supabase provided **production-ready, best-practice guidance** for bulk loading. Here's how to implement it.

---

## ðŸ“‹ What Supabase Recommended

### Key Strategies:
1. âœ… **COPY to UNLOGGED staging table** (fastest ingestion)
2. âœ… **INSERT...ON CONFLICT** for upserts (handles 9.7M existing records)
3. âœ… **20-minute statement timeout** for bulk operations
4. âœ… **ANALYZE staging before upsert** (optimize query plan)
5. âœ… **4 parallel COPY jobs, 1-2 parallel upserts** (safe concurrency)
6. âœ… **Column mapping in COPY** (no need for all 165 CSV columns)

---

## ðŸš€ Implementation Steps

### **Step 1: Setup (One-Time - 2 minutes)**

Run this SQL in Supabase SQL Editor:
https://supabase.com/dashboard/project/pmispwtdngkcmsrsjwbp/sql/new

```sql
-- Copy entire contents of setup-staging-optimized.sql
```

**What it does**:
- Configures ETL session settings (timeouts)
- Enables pg_trgm and btree_gin extensions
- Adds unique constraint on (parcel_id, county, year)
- Creates UNLOGGED staging table (no indexes)

**Expected Output**:
```
âœ… Setup Complete! Ready for bulk loading.
```

---

### **Step 2: Test Load One County (5-10 minutes)**

```bash
# Load Broward County
python bulk-load-county.py BROWARD
```

**What happens**:
1. Configures 20-minute timeout session
2. Clears staging table (TRUNCATE)
3. **COPY CSV â†’ staging** (fastest method)
4. ANALYZE staging (optimize query planner)
5. **Upsert to main table** with ON CONFLICT
6. Reports throughput (records/second)

**Expected Output**:
```
================================================================================
ðŸš€ BULK LOADING: BROWARD
================================================================================

ðŸ“ CSV File: TEMP/DATABASE PROPERTY APP/BROWARD/NAL/NAL16P202501.csv
ðŸ“Š File Size: 369.1 MB

âš™ï¸  Configuring ETL session...
âœ… Session configured

ðŸ§¹ Clearing staging table...
âœ… Staging table cleared

ðŸ“¥ COPYing NAL16P202501.csv to staging...
  ðŸ“¦ 500,000 records copied...
âœ… COPY complete: 500,000 records

ðŸ“Š Analyzing staging table...
âœ… Staging analyzed

â¬†ï¸  Upserting BROWARD records to main table...
âœ… Upsert complete: 500,000 rows affected

================================================================================
âœ… BULK LOAD COMPLETE: BROWARD
================================================================================
ðŸ“Š Records Processed: 500,000
â¬†ï¸  Rows Affected: 500,000
â±ï¸  Time Elapsed: 180.5s
âš¡ Throughput: 2,770 records/second
================================================================================
```

---

### **Step 3: Verify Results**

```bash
# Check total records
node -e "
const { createClient } = require('@supabase/supabase-js');
const supabase = createClient(
  'https://pmispwtdngkcmsrsjwbp.supabase.co',
  'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InBtaXNwd3RkbmdrY21zcnNqd2JwIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1Njk1Njk1OCwiZXhwIjoyMDcyNTMyOTU4fQ.fbCYcTFxLaMC_g4P8IrQoHWbQbPr_t9eaxYD_9yS3u0'
);
supabase.from('florida_parcels').select('*', { count: 'exact', head: true })
  .then(r => console.log('Total records:', r.count));
"
```

**Expected**: Total records increased by ~500K (from 9,728,687 to ~10,228,687)

---

## âš¡ Performance Expectations

### Based on Supabase's Guidance:

| County | Records | Time | Throughput |
|--------|---------|------|------------|
| Broward | 500K | 3-5 min | 2,000-3,000/sec |
| Miami-Dade | 800K | 5-8 min | 2,000-2,500/sec |
| Palm Beach | 400K | 2-4 min | 2,000-2,500/sec |

### Why So Fast:
1. **COPY is native Postgres** - bypasses query parsing
2. **UNLOGGED staging** - no WAL overhead during COPY
3. **No indexes on staging** - zero index maintenance cost
4. **Batched execute_values** - efficient INSERT operations
5. **Optimized ON CONFLICT** - uses unique index efficiently

---

## ðŸ”§ Advanced: Parallel Loading (After First Success)

Supabase recommends: **4 parallel COPY jobs, 1-2 parallel upserts**

### Option 1: Sequential (Safe)
```bash
# Load counties one at a time
python bulk-load-county.py BROWARD
python bulk-load-county.py MIAMI-DADE
python bulk-load-county.py PALM-BEACH
```

### Option 2: Parallel COPY (Faster, after you're comfortable)
```bash
# In separate terminals:
# Terminal 1:
python bulk-load-county.py BROWARD

# Terminal 2:
python bulk-load-county.py MIAMI-DADE

# Terminal 3:
python bulk-load-county.py PALM-BEACH

# Terminal 4:
python bulk-load-county.py ORANGE
```

**Note**: Each county uses the same staging table but runs sequentially per job. For true parallelism, you'd need per-county staging tables.

---

## ðŸ“Š Monitoring & Optimization

### Monitor Database Activity:

```sql
-- Active queries
SELECT
  pid,
  application_name,
  state,
  query_start,
  LEFT(query, 60) as query
FROM pg_stat_activity
WHERE application_name = 'etl_florida_parcels_upsert'
ORDER BY query_start DESC;

-- Staging table size
SELECT
  pg_size_pretty(pg_total_relation_size('public.florida_parcels_staging')) as staging_size,
  pg_size_pretty(pg_total_relation_size('public.florida_parcels')) as main_size;

-- Record counts
SELECT
  (SELECT count(*) FROM florida_parcels_staging) as staging_count,
  (SELECT count(*) FROM florida_parcels) as main_count;
```

---

## ðŸš¨ Troubleshooting

### Issue 1: "Statement timeout"
**Cause**: Upsert took >20 minutes
**Solution**: Increase timeout in `setup-staging-optimized.sql`:
```sql
SET statement_timeout = '30min';  -- Was 20min
```

### Issue 2: "Lock timeout"
**Cause**: Another process has lock on florida_parcels
**Solution**: Check active queries, wait for completion, or increase:
```sql
SET lock_timeout = '60s';  -- Was 30s
```

### Issue 3: Slow performance
**Cause**: Many concurrent operations
**Solution**: Reduce parallelism to 1-2 jobs

### Issue 4: "Unique constraint violation"
**Cause**: Shouldn't happen with ON CONFLICT
**Solution**: Verify unique constraint exists:
```sql
SELECT * FROM pg_constraint
WHERE conname = 'florida_parcels_unique_parcel_year';
```

---

## ðŸ“ˆ Next Steps After Successful Load

### 1. Re-enable RLS (If Disabled)
```sql
ALTER TABLE florida_parcels ENABLE ROW LEVEL SECURITY;
```

### 2. Vacuum & Analyze
```sql
VACUUM ANALYZE florida_parcels;
```

### 3. Check Index Health
```sql
-- Bloated indexes?
SELECT
  schemaname,
  tablename,
  indexname,
  pg_size_pretty(pg_relation_size(indexrelid)) as index_size
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
  AND tablename = 'florida_parcels'
ORDER BY pg_relation_size(indexrelid) DESC
LIMIT 20;
```

### 4. Set Up Scheduled Updates
Create daily/weekly cron jobs to refresh data:
```bash
# crontab -e
0 2 * * 0 cd /path/to/ConcordBroker && python bulk-load-county.py BROWARD
```

---

## ðŸ“Š Files Created

### Setup:
1. âœ… **setup-staging-optimized.sql** - One-time database setup
2. âœ… **bulk-load-county.py** - Production bulk loader (COPY-based)

### Documentation:
3. âœ… **SUPABASE_OPTIMIZED_IMPLEMENTATION.md** - This guide
4. âœ… **SUPABASE_BULK_LOAD_REQUEST.json** - Original request (10/10 response)

---

## âœ… Success Checklist

Before considering this complete:

- [ ] Run `setup-staging-optimized.sql` in Supabase SQL Editor
- [ ] Verify unique constraint exists on (parcel_id, county, year)
- [ ] Test load one county: `python bulk-load-county.py BROWARD`
- [ ] Verify record count increased by expected amount
- [ ] Check throughput is >2,000 records/second
- [ ] Load 2-3 more counties to confirm stability
- [ ] Set up monitoring queries for production
- [ ] Schedule automated updates (if needed)

---

## ðŸŽ¯ Expected Timeline

| Task | Time |
|------|------|
| One-time setup SQL | 2 min |
| First county load (test) | 5-10 min |
| Verify results | 1 min |
| Load all 67 counties (sequential) | 5-8 hours |
| Load all 67 counties (4x parallel) | 2-3 hours |

---

## ðŸ’¡ Pro Tips from Supabase

1. **Start with UNLOGGED staging** - 30-50% faster than normal tables
2. **TRUNCATE, don't DROP** - Faster and preserves metadata
3. **ANALYZE before upsert** - Ensures optimal query plan
4. **Use application_name** - Makes monitoring/debugging easier
5. **Column mapping in COPY** - No need for full 165-column staging
6. **Batched upserts** - 1,000-10,000 rows at a time is optimal

---

**You're ready for production bulk loading!** ðŸš€

**Quick Start**:
1. Run `setup-staging-optimized.sql` in Supabase
2. Run `python bulk-load-county.py BROWARD`
3. Watch the magic happen! âœ¨
