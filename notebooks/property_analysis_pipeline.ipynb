{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConcordBroker Property Analysis Pipeline\n",
    "## Comprehensive Data Analysis using SQLAlchemy, PySpark, OpenCV, and Playwright\n",
    "\n",
    "This notebook provides a complete data analysis pipeline for property data integration with the ConcordBroker website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "# SQLAlchemy for database operations\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from apps.api.models.sqlalchemy_models import (\n",
    "    DatabaseManager, FloridaParcel, TaxDeed, \n",
    "    SalesHistory, BuildingPermit, SunbizEntity,\n",
    "    PropertyImage, MarketAnalysis\n",
    ")\n",
    "\n",
    "# PySpark for big data processing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, sum, count, max, min\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# OpenCV for image analysis\n",
    "import cv2\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Playwright for web scraping\n",
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Database Connection with SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database connection\n",
    "db_manager = DatabaseManager()\n",
    "session = db_manager.get_session()\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    result = session.execute(text(\"SELECT COUNT(*) FROM florida_parcels\"))\n",
    "    count = result.scalar()\n",
    "    print(f\"✅ Connected to database. Total properties: {count:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Database connection error: {e}\")\n",
    "    \n",
    "# Query sample data\n",
    "sample_properties = session.query(FloridaParcel).limit(5).all()\n",
    "for prop in sample_properties:\n",
    "    print(f\"Property: {prop.parcel_id}, County: {prop.county}, Value: ${prop.just_value:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PySpark Setup for Big Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ConcordBroker_Analytics\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI: http://localhost:4040\")\n",
    "\n",
    "# Load property data into Spark DataFrame\n",
    "jdbc_url = f\"jdbc:postgresql://{os.getenv('POSTGRES_HOST')}/{os.getenv('POSTGRES_DATABASE')}\"\n",
    "properties_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"florida_parcels\") \\\n",
    "    .option(\"user\", os.getenv('POSTGRES_USER')) \\\n",
    "    .option(\"password\", os.getenv('POSTGRES_PASSWORD')) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "# Cache for performance\n",
    "properties_df.cache()\n",
    "print(f\"Total properties in Spark: {properties_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Property Market Analysis with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze property values by county\n",
    "county_analysis = properties_df.groupBy(\"county\").agg(\n",
    "    count(\"parcel_id\").alias(\"property_count\"),\n",
    "    avg(\"just_value\").alias(\"avg_value\"),\n",
    "    avg(\"land_value\").alias(\"avg_land_value\"),\n",
    "    avg(\"building_value\").alias(\"avg_building_value\"),\n",
    "    max(\"just_value\").alias(\"max_value\"),\n",
    "    min(\"just_value\").alias(\"min_value\")\n",
    ").orderBy(col(\"property_count\").desc())\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "county_stats = county_analysis.toPandas()\n",
    "\n",
    "# Visualize top counties\n",
    "fig = px.bar(county_stats.head(10), \n",
    "             x='county', y='property_count',\n",
    "             title='Top 10 Counties by Property Count',\n",
    "             labels={'property_count': 'Number of Properties'})\n",
    "fig.show()\n",
    "\n",
    "# Property value distribution\n",
    "value_distribution = properties_df.select(\"just_value\").filter(col(\"just_value\") < 1000000)\n",
    "value_pd = value_distribution.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(value_pd['just_value'], bins=50, edgecolor='black')\n",
    "plt.xlabel('Property Value ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Property Value Distribution (Under $1M)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning with PySpark - Property Value Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ML\n",
    "ml_data = properties_df.filter(\n",
    "    (col(\"land_sqft\").isNotNull()) & \n",
    "    (col(\"building_sqft\").isNotNull()) & \n",
    "    (col(\"year_built\").isNotNull()) &\n",
    "    (col(\"just_value\") > 0)\n",
    ").select(\n",
    "    \"land_sqft\", \"building_sqft\", \"year_built\", \n",
    "    \"bedrooms\", \"bathrooms\", \"just_value\"\n",
    ").na.fill(0)\n",
    "\n",
    "# Create feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"land_sqft\", \"building_sqft\", \"year_built\", \"bedrooms\", \"bathrooms\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "ml_data = assembler.transform(ml_data)\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = ml_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"just_value\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10\n",
    ")\n",
    "\n",
    "model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"just_value\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error: ${rmse:,.2f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': [\"land_sqft\", \"building_sqft\", \"year_built\", \"bedrooms\", \"bathrooms\"],\n",
    "    'importance': model.featureImportances.toArray()\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Playwright MCP Web Scraping Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertyWebScraper:\n",
    "    \"\"\"Web scraper using Playwright for property data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.playwright = None\n",
    "        self.browser = None\n",
    "        self.page = None\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Initialize Playwright browser\"\"\"\n",
    "        self.playwright = sync_playwright().start()\n",
    "        self.browser = self.playwright.chromium.launch(\n",
    "            headless=True,\n",
    "            args=['--disable-dev-shm-usage']\n",
    "        )\n",
    "        self.page = self.browser.new_page()\n",
    "    \n",
    "    def scrape_property_details(self, parcel_id: str, county: str) -> Dict:\n",
    "        \"\"\"Scrape property details from county website\"\"\"\n",
    "        try:\n",
    "            # Navigate to property appraiser site\n",
    "            url = f\"https://{county.lower()}.county-taxes.com/public/search?search_query={parcel_id}\"\n",
    "            self.page.goto(url, wait_until='networkidle')\n",
    "            \n",
    "            # Wait for content to load\n",
    "            self.page.wait_for_selector('.property-details', timeout=10000)\n",
    "            \n",
    "            # Extract data\n",
    "            data = {}\n",
    "            \n",
    "            # Property details\n",
    "            data['owner'] = self.page.query_selector('.owner-name')?.text_content()\n",
    "            data['address'] = self.page.query_selector('.property-address')?.text_content()\n",
    "            data['value'] = self.page.query_selector('.assessed-value')?.text_content()\n",
    "            \n",
    "            # Take screenshot for records\n",
    "            screenshot = self.page.screenshot()\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {parcel_id}: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def scrape_tax_deed_auction(self, auction_url: str) -> Dict:\n",
    "        \"\"\"Scrape tax deed auction information\"\"\"\n",
    "        try:\n",
    "            self.page.goto(auction_url, wait_until='networkidle')\n",
    "            \n",
    "            # Extract auction details\n",
    "            auction_data = {\n",
    "                'auction_date': self.page.query_selector('.auction-date')?.text_content(),\n",
    "                'minimum_bid': self.page.query_selector('.minimum-bid')?.text_content(),\n",
    "                'property_details': self.page.query_selector('.property-info')?.text_content(),\n",
    "                'bidders': []\n",
    "            }\n",
    "            \n",
    "            # Get bidder information\n",
    "            bidders = self.page.query_selector_all('.bidder-row')\n",
    "            for bidder in bidders:\n",
    "                auction_data['bidders'].append({\n",
    "                    'number': bidder.query_selector('.bidder-number')?.text_content(),\n",
    "                    'amount': bidder.query_selector('.bid-amount')?.text_content()\n",
    "                })\n",
    "            \n",
    "            return auction_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping auction: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close browser and cleanup\"\"\"\n",
    "        if self.browser:\n",
    "            self.browser.close()\n",
    "        if self.playwright:\n",
    "            self.playwright.stop()\n",
    "\n",
    "# Example usage\n",
    "scraper = PropertyWebScraper()\n",
    "scraper.start()\n",
    "\n",
    "# Scrape sample property\n",
    "# property_data = scraper.scrape_property_details(\"123456789\", \"BROWARD\")\n",
    "# print(\"Scraped property data:\", property_data)\n",
    "\n",
    "scraper.close()\n",
    "print(\"Web scraper initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. OpenCV Image Analysis for Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertyImageAnalyzer:\n",
    "    \"\"\"Analyze property images using OpenCV\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cascade_path = cv2.data.haarcascades\n",
    "    \n",
    "    def download_image(self, image_url: str) -> np.ndarray:\n",
    "        \"\"\"Download image from URL\"\"\"\n",
    "        response = requests.get(image_url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        return cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    def analyze_property_image(self, image_path: str) -> Dict:\n",
    "        \"\"\"Comprehensive image analysis\"\"\"\n",
    "        # Read image\n",
    "        if image_path.startswith('http'):\n",
    "            image = self.download_image(image_path)\n",
    "        else:\n",
    "            image = cv2.imread(image_path)\n",
    "        \n",
    "        analysis = {}\n",
    "        \n",
    "        # Basic properties\n",
    "        analysis['dimensions'] = image.shape[:2]\n",
    "        \n",
    "        # Color analysis\n",
    "        analysis['dominant_colors'] = self.get_dominant_colors(image)\n",
    "        \n",
    "        # Edge detection for structure analysis\n",
    "        edges = cv2.Canny(image, 100, 200)\n",
    "        analysis['edge_density'] = np.sum(edges > 0) / edges.size\n",
    "        \n",
    "        # Detect features\n",
    "        analysis['features'] = self.detect_features(image)\n",
    "        \n",
    "        # Quality assessment\n",
    "        analysis['quality_score'] = self.assess_image_quality(image)\n",
    "        \n",
    "        # Detect pools/water features\n",
    "        analysis['has_pool'] = self.detect_pool(image)\n",
    "        \n",
    "        # Vegetation analysis\n",
    "        analysis['vegetation_coverage'] = self.analyze_vegetation(image)\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def get_dominant_colors(self, image: np.ndarray, k: int = 5) -> List:\n",
    "        \"\"\"Extract dominant colors using K-means clustering\"\"\"\n",
    "        pixels = image.reshape((-1, 3))\n",
    "        pixels = np.float32(pixels)\n",
    "        \n",
    "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "        _, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "        \n",
    "        colors = centers.astype(int).tolist()\n",
    "        return colors\n",
    "    \n",
    "    def detect_features(self, image: np.ndarray) -> Dict:\n",
    "        \"\"\"Detect various features in the image\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect corners (buildings usually have many corners)\n",
    "        corners = cv2.goodFeaturesToTrack(gray, 100, 0.01, 10)\n",
    "        features['corner_count'] = len(corners) if corners is not None else 0\n",
    "        \n",
    "        # Detect lines (structural elements)\n",
    "        edges = cv2.Canny(gray, 50, 150)\n",
    "        lines = cv2.HoughLinesP(edges, 1, np.pi/180, 100, minLineLength=100, maxLineGap=10)\n",
    "        features['line_count'] = len(lines) if lines is not None else 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def assess_image_quality(self, image: np.ndarray) -> float:\n",
    "        \"\"\"Assess image quality based on various metrics\"\"\"\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Calculate Laplacian variance (sharpness)\n",
    "        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "        \n",
    "        # Normalize to 0-100 scale\n",
    "        quality_score = min(100, laplacian_var / 10)\n",
    "        \n",
    "        return quality_score\n",
    "    \n",
    "    def detect_pool(self, image: np.ndarray) -> bool:\n",
    "        \"\"\"Detect if image contains a pool\"\"\"\n",
    "        # Convert to HSV for better color detection\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Define blue color range for pool water\n",
    "        lower_blue = np.array([100, 50, 50])\n",
    "        upper_blue = np.array([130, 255, 255])\n",
    "        \n",
    "        # Create mask\n",
    "        mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "        \n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Check for pool-like shapes\n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area > 1000:  # Minimum area threshold\n",
    "                perimeter = cv2.arcLength(contour, True)\n",
    "                circularity = 4 * np.pi * area / (perimeter * perimeter)\n",
    "                if circularity > 0.5:  # Pool-like shape\n",
    "                    return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def analyze_vegetation(self, image: np.ndarray) -> float:\n",
    "        \"\"\"Analyze vegetation coverage in the image\"\"\"\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Define green color range\n",
    "        lower_green = np.array([35, 40, 40])\n",
    "        upper_green = np.array([85, 255, 255])\n",
    "        \n",
    "        # Create mask\n",
    "        mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "        \n",
    "        # Calculate percentage\n",
    "        vegetation_pixels = np.sum(mask > 0)\n",
    "        total_pixels = mask.size\n",
    "        \n",
    "        return (vegetation_pixels / total_pixels) * 100\n",
    "\n",
    "# Initialize analyzer\n",
    "image_analyzer = PropertyImageAnalyzer()\n",
    "print(\"OpenCV image analyzer initialized!\")\n",
    "\n",
    "# Example analysis (using a sample image)\n",
    "# analysis_result = image_analyzer.analyze_property_image(\"path_to_image.jpg\")\n",
    "# print(\"Image analysis results:\", analysis_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integrated Pipeline - Combining All Technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegratedPropertyPipeline:\n",
    "    \"\"\"Complete pipeline integrating all technologies\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.db_manager = DatabaseManager()\n",
    "        self.spark = spark  # Use existing Spark session\n",
    "        self.scraper = PropertyWebScraper()\n",
    "        self.image_analyzer = PropertyImageAnalyzer()\n",
    "        self.session = self.db_manager.get_session()\n",
    "    \n",
    "    def process_property(self, parcel_id: str, county: str) -> Dict:\n",
    "        \"\"\"Complete property processing pipeline\"\"\"\n",
    "        result = {'parcel_id': parcel_id, 'county': county}\n",
    "        \n",
    "        # 1. Get property from database (SQLAlchemy)\n",
    "        property_obj = self.session.query(FloridaParcel).filter_by(\n",
    "            parcel_id=parcel_id, county=county\n",
    "        ).first()\n",
    "        \n",
    "        if property_obj:\n",
    "            result['database'] = {\n",
    "                'owner': property_obj.owner_name,\n",
    "                'value': float(property_obj.just_value),\n",
    "                'address': property_obj.phy_addr1\n",
    "            }\n",
    "        \n",
    "        # 2. Scrape additional data (Playwright)\n",
    "        self.scraper.start()\n",
    "        scraped_data = self.scraper.scrape_property_details(parcel_id, county)\n",
    "        result['scraped'] = scraped_data\n",
    "        self.scraper.close()\n",
    "        \n",
    "        # 3. Analyze with PySpark\n",
    "        spark_analysis = self.analyze_property_spark(parcel_id, county)\n",
    "        result['spark_analysis'] = spark_analysis\n",
    "        \n",
    "        # 4. Process images if available (OpenCV)\n",
    "        if property_obj and property_obj.images:\n",
    "            image_results = []\n",
    "            for img in property_obj.images[:3]:  # Analyze up to 3 images\n",
    "                analysis = self.image_analyzer.analyze_property_image(img.image_url)\n",
    "                image_results.append(analysis)\n",
    "            result['image_analysis'] = image_results\n",
    "        \n",
    "        # 5. Generate investment score\n",
    "        result['investment_score'] = self.calculate_investment_score(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def analyze_property_spark(self, parcel_id: str, county: str) -> Dict:\n",
    "        \"\"\"Analyze property using Spark\"\"\"\n",
    "        # Get property and comparables\n",
    "        property_df = self.spark.sql(f\"\"\"\n",
    "            SELECT * FROM florida_parcels \n",
    "            WHERE parcel_id = '{parcel_id}' AND county = '{county}'\n",
    "        \"\"\")\n",
    "        \n",
    "        if property_df.count() == 0:\n",
    "            return {}\n",
    "        \n",
    "        prop = property_df.first()\n",
    "        \n",
    "        # Find comparable properties\n",
    "        comparables = self.spark.sql(f\"\"\"\n",
    "            SELECT * FROM florida_parcels\n",
    "            WHERE county = '{county}'\n",
    "            AND ABS(land_sqft - {prop.land_sqft}) < 1000\n",
    "            AND ABS(building_sqft - {prop.building_sqft}) < 500\n",
    "            AND parcel_id != '{parcel_id}'\n",
    "            LIMIT 10\n",
    "        \"\"\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = comparables.agg(\n",
    "            avg(\"just_value\").alias(\"avg_value\"),\n",
    "            avg(\"sale_price\").alias(\"avg_sale_price\"),\n",
    "            count(\"*\").alias(\"comp_count\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        return {\n",
    "            'comparable_avg_value': float(stats.avg_value) if stats.avg_value else 0,\n",
    "            'comparable_avg_sale': float(stats.avg_sale_price) if stats.avg_sale_price else 0,\n",
    "            'comparable_count': stats.comp_count,\n",
    "            'value_difference': float(prop.just_value - stats.avg_value) if stats.avg_value else 0\n",
    "        }\n",
    "    \n",
    "    def calculate_investment_score(self, property_data: Dict) -> float:\n",
    "        \"\"\"Calculate investment score based on all data\"\"\"\n",
    "        score = 50.0  # Base score\n",
    "        \n",
    "        # Database factors\n",
    "        if 'database' in property_data:\n",
    "            value = property_data['database'].get('value', 0)\n",
    "            if value > 0 and value < 500000:\n",
    "                score += 10  # Affordable range\n",
    "        \n",
    "        # Spark analysis factors\n",
    "        if 'spark_analysis' in property_data:\n",
    "            if property_data['spark_analysis'].get('value_difference', 0) < 0:\n",
    "                score += 15  # Below market value\n",
    "        \n",
    "        # Image analysis factors\n",
    "        if 'image_analysis' in property_data:\n",
    "            for img in property_data['image_analysis']:\n",
    "                if img.get('has_pool'):\n",
    "                    score += 5\n",
    "                if img.get('vegetation_coverage', 0) > 30:\n",
    "                    score += 3\n",
    "                if img.get('quality_score', 0) > 70:\n",
    "                    score += 2\n",
    "        \n",
    "        return min(100, score)  # Cap at 100\n",
    "    \n",
    "    def batch_process_properties(self, property_list: List[tuple]) -> pd.DataFrame:\n",
    "        \"\"\"Process multiple properties\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for parcel_id, county in property_list:\n",
    "            try:\n",
    "                result = self.process_property(parcel_id, county)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {parcel_id}: {e}\")\n",
    "                results.append({'parcel_id': parcel_id, 'error': str(e)})\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = IntegratedPropertyPipeline()\n",
    "print(\"Integrated pipeline initialized successfully!\")\n",
    "\n",
    "# Example processing\n",
    "# result = pipeline.process_property(\"123456789\", \"BROWARD\")\n",
    "# print(\"Property analysis result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. API Integration for Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI integration endpoint\n",
    "api_integration_code = '''\n",
    "from fastapi import FastAPI, HTTPException, Depends\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "import asyncio\n",
    "\n",
    "app = FastAPI(title=\"ConcordBroker Property API\")\n",
    "\n",
    "class PropertyRequest(BaseModel):\n",
    "    parcel_id: str\n",
    "    county: str\n",
    "    include_images: bool = True\n",
    "    include_spark: bool = True\n",
    "    include_scraping: bool = False\n",
    "\n",
    "class PropertyResponse(BaseModel):\n",
    "    parcel_id: str\n",
    "    county: str\n",
    "    owner: Optional[str]\n",
    "    value: Optional[float]\n",
    "    investment_score: float\n",
    "    analysis: dict\n",
    "    images: Optional[list]\n",
    "\n",
    "@app.post(\"/api/analyze-property\", response_model=PropertyResponse)\n",
    "async def analyze_property(request: PropertyRequest):\n",
    "    \"\"\"Comprehensive property analysis endpoint\"\"\"\n",
    "    try:\n",
    "        # Initialize pipeline\n",
    "        pipeline = IntegratedPropertyPipeline()\n",
    "        \n",
    "        # Process property\n",
    "        result = await asyncio.to_thread(\n",
    "            pipeline.process_property,\n",
    "            request.parcel_id,\n",
    "            request.county\n",
    "        )\n",
    "        \n",
    "        # Format response\n",
    "        return PropertyResponse(\n",
    "            parcel_id=result[\"parcel_id\"],\n",
    "            county=result[\"county\"],\n",
    "            owner=result.get(\"database\", {}).get(\"owner\"),\n",
    "            value=result.get(\"database\", {}).get(\"value\"),\n",
    "            investment_score=result.get(\"investment_score\", 0),\n",
    "            analysis=result.get(\"spark_analysis\", {}),\n",
    "            images=result.get(\"image_analysis\", [])\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/market-analysis/{county}\")\n",
    "async def market_analysis(county: str):\n",
    "    \"\"\"Get market analysis for a county\"\"\"\n",
    "    try:\n",
    "        # Use Spark for analysis\n",
    "        spark_df = spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                AVG(just_value) as avg_value,\n",
    "                AVG(sale_price) as avg_sale_price,\n",
    "                COUNT(*) as property_count,\n",
    "                AVG(YEAR(CURRENT_DATE) - year_built) as avg_age\n",
    "            FROM florida_parcels\n",
    "            WHERE county = \\'{county}\\'\n",
    "        \"\"\")\n",
    "        \n",
    "        stats = spark_df.collect()[0]\n",
    "        \n",
    "        return {\n",
    "            \"county\": county,\n",
    "            \"average_value\": float(stats.avg_value),\n",
    "            \"average_sale_price\": float(stats.avg_sale_price),\n",
    "            \"total_properties\": stats.property_count,\n",
    "            \"average_age\": float(stats.avg_age)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/image-analysis\")\n",
    "async def analyze_image(image_url: str):\n",
    "    \"\"\"Analyze property image using OpenCV\"\"\"\n",
    "    try:\n",
    "        analyzer = PropertyImageAnalyzer()\n",
    "        result = await asyncio.to_thread(\n",
    "            analyzer.analyze_property_image,\n",
    "            image_url\n",
    "        )\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Check API health and connections\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"database\": \"connected\",\n",
    "        \"spark\": \"running\",\n",
    "        \"services\": {\n",
    "            \"sqlalchemy\": True,\n",
    "            \"pyspark\": True,\n",
    "            \"opencv\": True,\n",
    "            \"playwright\": True\n",
    "        }\n",
    "    }\n",
    "'''\n",
    "\n",
    "# Save API code\n",
    "with open('../apps/api/integrated_property_api.py', 'w') as f:\n",
    "    f.write(api_integration_code)\n",
    "\n",
    "print(\"API integration code created successfully!\")\n",
    "print(\"\\nTo run the API:\")\n",
    "print(\"uvicorn apps.api.integrated_property_api:app --reload --port 8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-time Dashboard with Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive dashboard\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Get sample data for dashboard\n",
    "county_data = session.query(\n",
    "    FloridaParcel.county,\n",
    "    func.count(FloridaParcel.id).label('count'),\n",
    "    func.avg(FloridaParcel.just_value).label('avg_value')\n",
    ").group_by(FloridaParcel.county).limit(10).all()\n",
    "\n",
    "# Create dashboard\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Properties by County', 'Average Values', \n",
    "                   'Investment Scores', 'Market Trends'),\n",
    "    specs=[[{'type': 'bar'}, {'type': 'scatter'}],\n",
    "           [{'type': 'pie'}, {'type': 'scatter'}]]\n",
    ")\n",
    "\n",
    "# Properties by County\n",
    "counties = [c.county for c in county_data]\n",
    "counts = [c.count for c in county_data]\n",
    "fig.add_trace(\n",
    "    go.Bar(x=counties, y=counts, name='Property Count'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Average Values\n",
    "avg_values = [c.avg_value for c in county_data]\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=counties, y=avg_values, mode='lines+markers', name='Avg Value'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Investment Score Distribution (sample)\n",
    "investment_scores = ['Excellent', 'Good', 'Fair', 'Poor']\n",
    "score_counts = [25, 45, 20, 10]\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=investment_scores, values=score_counts),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Market Trends (sample)\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\n",
    "trend_values = [100, 105, 103, 108, 112, 115]\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=months, y=trend_values, mode='lines+markers', name='Market Trend'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"ConcordBroker Property Analytics Dashboard\",\n",
    "    showlegend=False,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Dashboard created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results and Save Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export analysis results\n",
    "export_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save Spark analysis to Parquet\n",
    "properties_df.write.mode('overwrite').parquet(f'analysis_results/properties_{export_timestamp}.parquet')\n",
    "\n",
    "# Save SQLAlchemy query results to CSV\n",
    "query_results = pd.read_sql(\n",
    "    \"SELECT * FROM florida_parcels LIMIT 1000\",\n",
    "    con=db_manager.engine\n",
    ")\n",
    "query_results.to_csv(f'analysis_results/sample_properties_{export_timestamp}.csv', index=False)\n",
    "\n",
    "# Save analysis summary\n",
    "summary = {\n",
    "    'timestamp': export_timestamp,\n",
    "    'total_properties_analyzed': properties_df.count(),\n",
    "    'counties_covered': properties_df.select('county').distinct().count(),\n",
    "    'technologies_used': [\n",
    "        'SQLAlchemy',\n",
    "        'PySpark',\n",
    "        'OpenCV',\n",
    "        'Playwright',\n",
    "        'Jupyter'\n",
    "    ],\n",
    "    'api_endpoints': [\n",
    "        '/api/analyze-property',\n",
    "        '/api/market-analysis',\n",
    "        '/api/image-analysis',\n",
    "        '/api/health'\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(f'analysis_results/summary_{export_timestamp}.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"✅ Analysis results exported to analysis_results/ directory\")\n",
    "print(f\"Timestamp: {export_timestamp}\")\n",
    "\n",
    "# Close connections\n",
    "session.close()\n",
    "spark.stop()\n",
    "print(\"\\n✅ All connections closed. Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}