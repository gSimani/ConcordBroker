# üöÄ OPTIMIZED DEPLOYMENT GUIDE (Based on Supabase Feedback)

**Created:** October 1, 2025
**Status:** READY TO DEPLOY
**File:** `DEPLOY_NOW_BULK_DOR_OPTIMIZED.sql`

---

## üéØ KEY OPTIMIZATIONS IMPLEMENTED

### **1. Direct UPDATE (No CTE Materialization)**
**Before (our approach):**
```sql
WITH calculated_codes AS (
    SELECT id, CASE WHEN ... END as code
    FROM florida_parcels WHERE year = 2025 AND county = 'DADE'
)
UPDATE florida_parcels p
SET land_use_code = c.code
FROM calculated_codes c
WHERE p.id = c.id;
```

**After (Supabase recommendation):**
```sql
UPDATE florida_parcels
SET land_use_code = CASE WHEN building_value > 500000 THEN '02' ELSE '00' END
WHERE year = 2025 AND county = 'DADE'
  AND land_use_code IS DISTINCT FROM CASE WHEN ... END;
```

**Why Better:**
- Avoids CTE materialization overhead
- Postgres optimizer can use index more efficiently
- Less memory usage

---

### **2. IS DISTINCT FROM (Skip No-Op Writes)**
```sql
WHERE ...
  AND (
      land_use_code IS DISTINCT FROM CASE ... END
      OR property_use IS DISTINCT FROM CASE ... END
      OR land_use_code IS NULL
      OR property_use IS NULL
  )
```

**Impact:**
- Only updates rows where values actually change
- Reduces WAL (Write-Ahead Log) volume
- Faster execution (fewer disk writes)
- Less bloat generation

---

### **3. Explicit COMMIT Between Counties**
```sql
UPDATE florida_parcels ... WHERE county = 'DADE';
COMMIT;

UPDATE florida_parcels ... WHERE county = 'BROWARD';
COMMIT;
```

**Benefits:**
- Smaller transaction sizes (reduces WAL pressure)
- Partial success possible (if county 10 fails, counties 1-9 are saved)
- Earlier visibility of updates to concurrent queries
- Better checkpoint management

---

### **4. Pre-Flight Index Check**
```sql
-- Automatically creates idx_parcels_year_county if missing
IF NOT EXISTS (...) THEN
    CREATE INDEX CONCURRENTLY idx_parcels_year_county
    ON florida_parcels(year, county);
END IF;
```

**Why Critical:**
- Required for efficient `WHERE year = 2025 AND county = 'DADE'` filtering
- Without it: Full table scan of 9.1M rows per county
- With it: Index scan of ~450K rows per county

---

### **5. Auto VACUUM ANALYZE**
```sql
VACUUM ANALYZE florida_parcels;
```

**Included automatically after bulk update:**
- Reclaims space from deleted row versions
- Updates table statistics for query planner
- Estimated time: 5-20 minutes
- Does NOT lock table

---

## üöÄ DEPLOYMENT STEPS

### **Step 1: Open Supabase SQL Editor**
1. Go to your Supabase Dashboard
2. Click **SQL Editor** (left sidebar)
3. Click **New Query**

---

### **Step 2: Copy & Paste Script**
1. Open `DEPLOY_NOW_BULK_DOR_OPTIMIZED.sql`
2. Copy entire file (Ctrl+A, Ctrl+C)
3. Paste into Supabase SQL Editor (Ctrl+V)

---

### **Step 3: Run Script**
1. Click **RUN** button (bottom right)
2. Switch to **Messages** tab to monitor progress

---

### **Step 4: Monitor Progress**

You'll see messages like:

```
NOTICE: ========================================
NOTICE: BULK DOR CODE ASSIGNMENT STARTED
NOTICE: Started at: 2025-10-01 09:00:00
NOTICE: Optimization: Direct UPDATE + IS DISTINCT FROM
NOTICE: ========================================

NOTICE: ‚úì Required index idx_parcels_year_county already exists

NOTICE: Processing: DADE
NOTICE:   ‚úì DADE           :  2,300,000 properties updated in 62.34 seconds

NOTICE: Processing: BROWARD
NOTICE:   ‚úì BROWARD        :    800,000 properties updated in 24.12 seconds

NOTICE: Processing: PALM BEACH
NOTICE:   ‚úì PALM BEACH     :    600,000 properties updated in 18.45 seconds

... (continues for all 20 counties)

NOTICE: ========================================
NOTICE: BULK DOR CODE ASSIGNMENT COMPLETE!
NOTICE: Total properties updated: 9,113,150
NOTICE: Completed at: 2025-10-01 09:12:00
NOTICE: ========================================

NOTICE: Running VACUUM ANALYZE on florida_parcels...
NOTICE: ‚úì VACUUM ANALYZE complete

NOTICE: Running verification queries...
```

---

### **Step 5: Review Results**

After completion, you'll see 3 verification tables:

**Table 1: By County**
```
county          | total     | coded     | pct_coded
----------------|-----------|-----------|----------
DADE            | 2,300,000 | 2,300,000 | 100.0
BROWARD         |   800,000 |   800,000 | 100.0
...
```

**Table 2: By DOR Code**
```
land_use_code | property_use | count     | avg_value
--------------|--------------|-----------|----------
00            | SFR          | 5,500,000 | 285,000
02            | MF 10+       |   450,000 | 820,000
17            | Commercia    |   380,000 | 1,200,000
...
```

**Table 3: Overall Summary**
```
total_properties | coded_properties | uncoded_properties | pct_complete
-----------------|------------------|--------------------|--------------
9,113,150        | 9,113,150        | 0                  | 100.00
```

---

## ‚è±Ô∏è EXPECTED TIMELINE

| Phase | Duration | Description |
|-------|----------|-------------|
| Index check | <10 sec | Verify/create idx_parcels_year_county |
| DADE county | 60-90 sec | ~2.3M properties (largest) |
| BROWARD | 20-30 sec | ~800K properties |
| Other 18 counties | 5-30 sec each | ~100K-600K each |
| **Total updates** | **8-12 min** | All 9.1M properties |
| VACUUM ANALYZE | 5-20 min | Reclaim space, update stats |
| **Grand Total** | **13-32 min** | Complete deployment |

---

## ‚ö†Ô∏è TROUBLESHOOTING

### **Error: "Statement timeout"**
**Solution:**
```sql
-- Run this first, then retry:
SET statement_timeout = '30min';
```

---

### **Error: "Out of memory"**
**Solution:**
```sql
-- Increase work memory:
SET work_mem = '512MB';
```

---

### **Error: "Database is locked"**
**Cause:** Another heavy query is running
**Solution:** Wait 2-5 minutes and retry, or check `pg_stat_activity`:
```sql
SELECT pid, query, state, wait_event
FROM pg_stat_activity
WHERE state = 'active' AND pid != pg_backend_pid();
```

---

### **Slow Performance (>30 min total)**
**Check:**
1. Is the index `idx_parcels_year_county` present?
   ```sql
   SELECT indexname FROM pg_indexes
   WHERE tablename = 'florida_parcels' AND indexname = 'idx_parcels_year_county';
   ```
2. Are there heavy concurrent queries?
   ```sql
   SELECT COUNT(*) FROM pg_stat_activity WHERE state = 'active';
   ```

---

## ‚úÖ SUCCESS CRITERIA

After deployment, verify:

- ‚úÖ All 20 counties processed (see progress messages)
- ‚úÖ Total updated: ~9.1M properties
- ‚úÖ pct_complete: 100.00%
- ‚úÖ No errors in Messages tab
- ‚úÖ VACUUM ANALYZE completed

---

## üìä PERFORMANCE COMPARISON

| Metric | Old Method | Optimized Method | Improvement |
|--------|------------|------------------|-------------|
| Approach | Individual API calls | Bulk SQL UPDATE | - |
| Runtime | 40+ days | 10-15 minutes | **5,760x** |
| WAL volume | High (9.1M transactions) | Low (20 transactions) | **455,000x** |
| Database load | Constant exhaustion | Brief spike | - |
| Rollback risk | All-or-nothing | Per-county | - |
| No-op writes | All rows touched | Only changed rows | 2-5x fewer writes |

---

## üéØ NEXT STEPS AFTER DEPLOYMENT

### **Immediate (Same Session)**
1. ‚úÖ Review verification results
2. ‚úÖ Confirm 100% completion
3. ‚úÖ Check for any error messages

### **Same Day**
1. Deploy `add_critical_missing_indexes.sql` (15-20 min)
2. Deploy `create_staging_tables.sql` (1 min)
3. Deploy `merge_staging_to_production.sql` (1 min)

### **This Week**
1. Deploy `create_filter_optimized_view.sql` (10-40 min)
2. Deploy `optimize_trigram_text_search.sql` (15-30 min)
3. Update Python scripts to use staging pattern

---

## üìû IF YOU NEED HELP

**Check Status:**
```sql
-- See what's running
SELECT pid, state, query, now() - query_start as runtime
FROM pg_stat_activity
WHERE datname = current_database() AND state = 'active'
ORDER BY runtime DESC;
```

**Monitor Progress (During Run):**
```sql
-- Refresh this every 30 seconds
SELECT schemaname, tablename, n_tup_upd, n_tup_hot_upd
FROM pg_stat_user_tables
WHERE tablename = 'florida_parcels';
-- n_tup_upd will increment as updates happen
```

---

## üéâ READY TO DEPLOY!

**File:** `DEPLOY_NOW_BULK_DOR_OPTIMIZED.sql`
**Action:** Copy into Supabase SQL Editor ‚Üí Click RUN
**Time:** ~10-15 minutes for updates + 5-20 minutes for VACUUM
**Impact:** 5,760x faster than current method

---

**Good luck! Report back with results or any errors.** üöÄ
