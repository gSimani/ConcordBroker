#!/usr/bin/env python3
"""
Specialized AI Agents for Continuous Data Flow Monitoring
Individual agents that monitor specific aspects of the ConcordBroker system
"""

import asyncio
import logging
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable
from abc import ABC, abstractmethod
import traceback
from dataclasses import dataclass, asdict

# AI and ML imports
import openai
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.tools import Tool
from langchain.prompts import ChatPromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.schema import BaseMessage, HumanMessage, AIMessage

# Database and async support
import asyncpg
import aioredis
from sqlalchemy.ext.asyncio import AsyncSession
import pandas as pd
import numpy as np

# Local imports
from sqlalchemy_models import (
    DatabaseManager, PropertyDataOperations, EntityOperations, MonitoringOperations,
    FloridaParcels, PropertySalesHistory, TaxCertificates, DataFlowMetrics, ValidationResults
)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AgentAlert:
    """Represents an alert generated by an agent"""
    agent_name: str
    alert_type: str
    severity: str  # LOW, MEDIUM, HIGH, CRITICAL
    message: str
    details: Dict[str, Any]
    timestamp: datetime
    resolved: bool = False

@dataclass
class AgentStatus:
    """Represents the status of an agent"""
    agent_name: str
    status: str  # ACTIVE, INACTIVE, ERROR, MAINTENANCE
    last_check: datetime
    next_check: datetime
    metrics: Dict[str, Any]
    alerts_count: int

class BaseMonitoringAgent(ABC):
    """Base class for all monitoring agents"""

    def __init__(
        self,
        name: str,
        check_interval: int = 300,  # 5 minutes default
        db_manager: DatabaseManager = None,
        openai_api_key: str = None
    ):
        self.name = name
        self.check_interval = check_interval
        self.db_manager = db_manager
        self.openai_api_key = openai_api_key
        self.status = "INACTIVE"
        self.last_check = None
        self.next_check = None
        self.alerts = []
        self.metrics = {}
        self.monitoring_active = False
        self.llm = None

        if openai_api_key:
            self.llm = ChatOpenAI(
                api_key=openai_api_key,
                model="gpt-4",
                temperature=0.1
            )

    @abstractmethod
    async def check_health(self) -> Dict[str, Any]:
        """Perform health check specific to this agent"""
        pass

    @abstractmethod
    async def analyze_data(self) -> Dict[str, Any]:
        """Analyze data specific to this agent's domain"""
        pass

    async def generate_alert(
        self,
        alert_type: str,
        severity: str,
        message: str,
        details: Dict[str, Any] = None
    ) -> AgentAlert:
        """Generate an alert"""
        alert = AgentAlert(
            agent_name=self.name,
            alert_type=alert_type,
            severity=severity,
            message=message,
            details=details or {},
            timestamp=datetime.now()
        )
        self.alerts.append(alert)
        logger.warning(f"🚨 {self.name} Alert [{severity}]: {message}")
        return alert

    async def resolve_alert(self, alert: AgentAlert):
        """Mark an alert as resolved"""
        alert.resolved = True
        logger.info(f"✅ {self.name} Alert resolved: {alert.message}")

    async def get_ai_insight(self, data: Dict[str, Any], question: str) -> str:
        """Get AI insight on data"""
        if not self.llm:
            return "AI insights not available"

        try:
            prompt = f"""
            As an expert data analyst for ConcordBroker, analyze the following data and answer the question.

            Data Context: {json.dumps(data, default=str, indent=2)}

            Question: {question}

            Provide a concise, actionable insight focusing on:
            1. What the data tells us
            2. Any patterns or anomalies
            3. Recommended actions if any issues are found
            4. Risk assessment

            Keep the response under 200 words and be specific.
            """

            response = await self.llm.ainvoke([HumanMessage(content=prompt)])
            return response.content

        except Exception as e:
            logger.error(f"AI insight generation failed: {e}")
            return f"AI analysis failed: {str(e)}"

    async def start_monitoring(self):
        """Start continuous monitoring"""
        self.monitoring_active = True
        self.status = "ACTIVE"
        logger.info(f"🚀 {self.name} monitoring started")

        try:
            while self.monitoring_active:
                self.last_check = datetime.now()
                self.next_check = self.last_check + timedelta(seconds=self.check_interval)

                try:
                    # Perform health check
                    health_data = await self.check_health()
                    self.metrics.update(health_data)

                    # Analyze data
                    analysis_data = await self.analyze_data()
                    self.metrics.update(analysis_data)

                    # Log successful check
                    if self.db_manager:
                        monitoring_ops = MonitoringOperations(self.db_manager)
                        monitoring_ops.log_metric(
                            table_name=f"agent_{self.name.lower()}",
                            metric_type="health_check",
                            metric_value=1.0,
                            metric_unit="success",
                            additional_context={"status": self.status, "alerts": len(self.alerts)}
                        )

                except Exception as e:
                    logger.error(f"❌ {self.name} monitoring error: {e}")
                    await self.generate_alert(
                        alert_type="MONITORING_ERROR",
                        severity="HIGH",
                        message=f"Monitoring check failed: {str(e)}",
                        details={"error": str(e), "traceback": traceback.format_exc()}
                    )

                # Wait for next check
                sleep_time = max(1, self.check_interval - (datetime.now() - self.last_check).total_seconds())
                await asyncio.sleep(sleep_time)

        except asyncio.CancelledError:
            logger.info(f"🛑 {self.name} monitoring cancelled")
        except Exception as e:
            logger.error(f"❌ {self.name} monitoring failed: {e}")
            self.status = "ERROR"
        finally:
            self.status = "INACTIVE"

    async def stop_monitoring(self):
        """Stop monitoring"""
        self.monitoring_active = False
        logger.info(f"🛑 {self.name} monitoring stopped")

    def get_status(self) -> AgentStatus:
        """Get current agent status"""
        return AgentStatus(
            agent_name=self.name,
            status=self.status,
            last_check=self.last_check,
            next_check=self.next_check,
            metrics=self.metrics,
            alerts_count=len([a for a in self.alerts if not a.resolved])
        )

class PropertyDataAgent(BaseMonitoringAgent):
    """Agent monitoring property data quality and availability"""

    def __init__(self, **kwargs):
        super().__init__("PropertyDataAgent", **kwargs)
        self.property_ops = None
        if self.db_manager:
            self.property_ops = PropertyDataOperations(self.db_manager)

    async def check_health(self) -> Dict[str, Any]:
        """Check property data health"""
        try:
            if not self.db_manager:
                return {"error": "Database manager not available"}

            # Check florida_parcels table
            with self.db_manager.get_session() as session:
                # Count total properties
                total_properties = session.query(FloridaParcels).count()

                # Count by county
                county_counts = session.query(
                    FloridaParcels.county,
                    session.query(FloridaParcels).filter(
                        FloridaParcels.county == FloridaParcels.county
                    ).count().label('count')
                ).group_by(FloridaParcels.county).limit(10).all()

                # Check for recent updates
                recent_updates = session.query(FloridaParcels).filter(
                    FloridaParcels.updated_at >= datetime.now() - timedelta(days=7)
                ).count()

                # Check data quality metrics
                null_owner_count = session.query(FloridaParcels).filter(
                    FloridaParcels.owner_name.is_(None)
                ).count()

                null_value_count = session.query(FloridaParcels).filter(
                    FloridaParcels.just_value.is_(None)
                ).count()

            health_data = {
                "total_properties": total_properties,
                "recent_updates": recent_updates,
                "null_owner_percentage": (null_owner_count / total_properties * 100) if total_properties > 0 else 0,
                "null_value_percentage": (null_value_count / total_properties * 100) if total_properties > 0 else 0,
                "county_distribution": {county: count for county, count in county_counts}
            }

            # Generate alerts based on thresholds
            if health_data["null_owner_percentage"] > 10:
                await self.generate_alert(
                    alert_type="DATA_QUALITY",
                    severity="MEDIUM",
                    message=f"High null owner percentage: {health_data['null_owner_percentage']:.1f}%",
                    details={"threshold": 10, "actual": health_data["null_owner_percentage"]}
                )

            if health_data["null_value_percentage"] > 5:
                await self.generate_alert(
                    alert_type="DATA_QUALITY",
                    severity="HIGH",
                    message=f"High null value percentage: {health_data['null_value_percentage']:.1f}%",
                    details={"threshold": 5, "actual": health_data["null_value_percentage"]}
                )

            if recent_updates == 0:
                await self.generate_alert(
                    alert_type="DATA_STALENESS",
                    severity="MEDIUM",
                    message="No property updates in the last 7 days",
                    details={"days_without_updates": 7}
                )

            return health_data

        except Exception as e:
            logger.error(f"Property data health check failed: {e}")
            return {"error": str(e)}

    async def analyze_data(self) -> Dict[str, Any]:
        """Analyze property data patterns"""
        try:
            if not self.property_ops:
                return {"error": "Property operations not available"}

            # Sample properties for analysis
            sample_properties = self.property_ops.search_properties(limit=1000)

            if not sample_properties:
                return {"error": "No properties found for analysis"}

            # Convert to DataFrame for analysis
            property_data = []
            for prop in sample_properties:
                property_data.append({
                    'county': prop.county,
                    'just_value': float(prop.just_value) if prop.just_value else 0,
                    'land_value': float(prop.land_value) if prop.land_value else 0,
                    'building_value': float(prop.building_value) if prop.building_value else 0,
                    'total_living_area': prop.total_living_area or 0,
                    'year_built': prop.year_built or 0,
                    'beds': prop.beds or 0,
                    'baths': float(prop.baths) if prop.baths else 0
                })

            df = pd.DataFrame(property_data)

            # Perform analysis
            analysis = {
                "sample_size": len(df),
                "value_statistics": {
                    "mean_just_value": df['just_value'].mean(),
                    "median_just_value": df['just_value'].median(),
                    "std_just_value": df['just_value'].std(),
                    "min_just_value": df['just_value'].min(),
                    "max_just_value": df['just_value'].max()
                },
                "property_characteristics": {
                    "mean_living_area": df['total_living_area'].mean(),
                    "mean_year_built": df['year_built'].mean(),
                    "mean_beds": df['beds'].mean(),
                    "mean_baths": df['baths'].mean()
                },
                "outliers": {
                    "high_value_properties": len(df[df['just_value'] > df['just_value'].quantile(0.99)]),
                    "zero_value_properties": len(df[df['just_value'] == 0])
                }
            }

            # Get AI insight
            ai_insight = await self.get_ai_insight(
                analysis,
                "What patterns do you see in this property data? Are there any concerns about data quality or outliers?"
            )
            analysis["ai_insight"] = ai_insight

            return analysis

        except Exception as e:
            logger.error(f"Property data analysis failed: {e}")
            return {"error": str(e)}

class SalesDataAgent(BaseMonitoringAgent):
    """Agent monitoring sales data and market trends"""

    def __init__(self, **kwargs):
        super().__init__("SalesDataAgent", **kwargs)

    async def check_health(self) -> Dict[str, Any]:
        """Check sales data health"""
        try:
            if not self.db_manager:
                return {"error": "Database manager not available"}

            with self.db_manager.get_session() as session:
                # Total sales count
                total_sales = session.query(PropertySalesHistory).count()

                # Recent sales (last 30 days)
                recent_sales = session.query(PropertySalesHistory).filter(
                    PropertySalesHistory.sale_date >= datetime.now().date() - timedelta(days=30)
                ).count()

                # Sales with zero price (potential data quality issue)
                zero_price_sales = session.query(PropertySalesHistory).filter(
                    PropertySalesHistory.sale_price <= 0
                ).count()

                # Sales missing dates
                null_date_sales = session.query(PropertySalesHistory).filter(
                    PropertySalesHistory.sale_date.is_(None)
                ).count()

                # Average sale price (recent)
                recent_avg_price = session.query(
                    session.query(PropertySalesHistory.sale_price).filter(
                        PropertySalesHistory.sale_date >= datetime.now().date() - timedelta(days=90),
                        PropertySalesHistory.sale_price > 0
                    ).all()
                )

            health_data = {
                "total_sales": total_sales,
                "recent_sales_30d": recent_sales,
                "zero_price_percentage": (zero_price_sales / total_sales * 100) if total_sales > 0 else 0,
                "null_date_percentage": (null_date_sales / total_sales * 100) if total_sales > 0 else 0,
                "data_quality_score": 100 - (zero_price_sales + null_date_sales) / total_sales * 100 if total_sales > 0 else 0
            }

            # Generate alerts
            if health_data["zero_price_percentage"] > 5:
                await self.generate_alert(
                    alert_type="DATA_QUALITY",
                    severity="MEDIUM",
                    message=f"High percentage of zero-price sales: {health_data['zero_price_percentage']:.1f}%",
                    details={"threshold": 5, "actual": health_data["zero_price_percentage"]}
                )

            if health_data["recent_sales_30d"] == 0:
                await self.generate_alert(
                    alert_type="DATA_STALENESS",
                    severity="HIGH",
                    message="No sales recorded in the last 30 days",
                    details={"period": "30 days"}
                )

            return health_data

        except Exception as e:
            logger.error(f"Sales data health check failed: {e}")
            return {"error": str(e)}

    async def analyze_data(self) -> Dict[str, Any]:
        """Analyze sales trends and patterns"""
        try:
            if not self.db_manager:
                return {"error": "Database manager not available"}

            with self.db_manager.get_session() as session:
                # Get recent sales for analysis
                recent_sales = session.query(PropertySalesHistory).filter(
                    PropertySalesHistory.sale_date >= datetime.now().date() - timedelta(days=365),
                    PropertySalesHistory.sale_price > 0
                ).limit(5000).all()

                if not recent_sales:
                    return {"error": "No recent sales data available"}

                # Convert to DataFrame
                sales_data = []
                for sale in recent_sales:
                    sales_data.append({
                        'sale_date': sale.sale_date,
                        'sale_price': float(sale.sale_price),
                        'month': sale.sale_date.month if sale.sale_date else None,
                        'year': sale.sale_date.year if sale.sale_date else None
                    })

                df = pd.DataFrame(sales_data)
                df = df.dropna()

                # Market analysis
                analysis = {
                    "sample_size": len(df),
                    "price_trends": {
                        "mean_price": df['sale_price'].mean(),
                        "median_price": df['sale_price'].median(),
                        "price_std": df['sale_price'].std(),
                        "min_price": df['sale_price'].min(),
                        "max_price": df['sale_price'].max()
                    },
                    "temporal_patterns": {
                        "monthly_distribution": df.groupby('month')['sale_price'].count().to_dict(),
                        "monthly_avg_price": df.groupby('month')['sale_price'].mean().to_dict()
                    },
                    "market_indicators": {
                        "coefficient_of_variation": df['sale_price'].std() / df['sale_price'].mean(),
                        "price_volatility": "High" if df['sale_price'].std() / df['sale_price'].mean() > 0.5 else "Normal"
                    }
                }

                # Get AI insight
                ai_insight = await self.get_ai_insight(
                    analysis,
                    "What do these sales trends indicate about the current real estate market? Are there any concerning patterns?"
                )
                analysis["ai_insight"] = ai_insight

                return analysis

        except Exception as e:
            logger.error(f"Sales data analysis failed: {e}")
            return {"error": str(e)}

class TaxCertificateAgent(BaseMonitoringAgent):
    """Agent monitoring tax certificate data and patterns"""

    def __init__(self, **kwargs):
        super().__init__("TaxCertificateAgent", **kwargs)

    async def check_health(self) -> Dict[str, Any]:
        """Check tax certificate data health"""
        try:
            if not self.db_manager:
                return {"error": "Database manager not available"}

            with self.db_manager.get_session() as session:
                # Total certificates
                total_certs = session.query(TaxCertificates).count()

                # Active certificates
                active_certs = session.query(TaxCertificates).filter(
                    TaxCertificates.status == 'Active'
                ).count()

                # Recent certificates (current year)
                current_year = datetime.now().year
                recent_certs = session.query(TaxCertificates).filter(
                    TaxCertificates.certificate_year == current_year
                ).count()

                # Certificates by status
                status_counts = {}
                statuses = session.query(TaxCertificates.status).distinct().all()
                for status_tuple in statuses:
                    status = status_tuple[0]
                    count = session.query(TaxCertificates).filter(
                        TaxCertificates.status == status
                    ).count()
                    status_counts[status] = count

                # Average certificate amount
                avg_amount = session.query(
                    session.query(TaxCertificates.certificate_amount).filter(
                        TaxCertificates.certificate_amount > 0
                    ).all()
                )

            health_data = {
                "total_certificates": total_certs,
                "active_certificates": active_certs,
                "current_year_certificates": recent_certs,
                "status_distribution": status_counts,
                "active_percentage": (active_certs / total_certs * 100) if total_certs > 0 else 0
            }

            # Generate alerts
            if health_data["active_percentage"] > 20:
                await self.generate_alert(
                    alert_type="HIGH_TAX_ACTIVITY",
                    severity="MEDIUM",
                    message=f"High percentage of active tax certificates: {health_data['active_percentage']:.1f}%",
                    details={"threshold": 20, "actual": health_data["active_percentage"]}
                )

            return health_data

        except Exception as e:
            logger.error(f"Tax certificate health check failed: {e}")
            return {"error": str(e)}

    async def analyze_data(self) -> Dict[str, Any]:
        """Analyze tax certificate patterns"""
        try:
            if not self.db_manager:
                return {"error": "Database manager not available"}

            with self.db_manager.get_session() as session:
                # Get certificates for analysis
                certificates = session.query(TaxCertificates).limit(2000).all()

                if not certificates:
                    return {"error": "No tax certificate data available"}

                # Convert to DataFrame
                cert_data = []
                for cert in certificates:
                    cert_data.append({
                        'county': cert.county,
                        'certificate_year': cert.certificate_year,
                        'certificate_amount': float(cert.certificate_amount),
                        'status': cert.status
                    })

                df = pd.DataFrame(cert_data)

                # Analysis
                analysis = {
                    "sample_size": len(df),
                    "amount_statistics": {
                        "mean_amount": df['certificate_amount'].mean(),
                        "median_amount": df['certificate_amount'].median(),
                        "total_amount": df['certificate_amount'].sum(),
                        "max_amount": df['certificate_amount'].max()
                    },
                    "temporal_trends": {
                        "yearly_distribution": df.groupby('certificate_year').size().to_dict(),
                        "yearly_total_amount": df.groupby('certificate_year')['certificate_amount'].sum().to_dict()
                    },
                    "geographic_distribution": {
                        "county_counts": df.groupby('county').size().to_dict(),
                        "county_total_amounts": df.groupby('county')['certificate_amount'].sum().to_dict()
                    },
                    "status_analysis": {
                        "status_distribution": df.groupby('status').size().to_dict(),
                        "status_amounts": df.groupby('status')['certificate_amount'].sum().to_dict()
                    }
                }

                # Get AI insight
                ai_insight = await self.get_ai_insight(
                    analysis,
                    "What patterns do you observe in the tax certificate data? Are there any areas of concern or investment opportunities?"
                )
                analysis["ai_insight"] = ai_insight

                return analysis

        except Exception as e:
            logger.error(f"Tax certificate analysis failed: {e}")
            return {"error": str(e)}

class EntityLinkingAgent(BaseMonitoringAgent):
    """Agent monitoring entity linking and data consistency"""

    def __init__(self, **kwargs):
        super().__init__("EntityLinkingAgent", **kwargs)
        self.entity_ops = None
        if self.db_manager:
            self.entity_ops = EntityOperations(self.db_manager)

    async def check_health(self) -> Dict[str, Any]:
        """Check entity data health"""
        try:
            if not self.db_manager:
                return {"error": "Database manager not available"}

            from sqlalchemy_models import FloridaEntities, SunbizCorporate

            with self.db_manager.get_session() as session:
                # Florida entities count
                florida_entities_count = session.query(FloridaEntities).count()

                # Sunbiz entities count
                sunbiz_count = session.query(SunbizCorporate).count()

                # Active entities
                active_florida = session.query(FloridaEntities).filter(
                    FloridaEntities.status == 'Active'
                ).count()

                active_sunbiz = session.query(SunbizCorporate).filter(
                    SunbizCorporate.status == 'Active'
                ).count()

            health_data = {
                "florida_entities_total": florida_entities_count,
                "sunbiz_entities_total": sunbiz_count,
                "total_entities": florida_entities_count + sunbiz_count,
                "active_florida_entities": active_florida,
                "active_sunbiz_entities": active_sunbiz,
                "florida_active_percentage": (active_florida / florida_entities_count * 100) if florida_entities_count > 0 else 0,
                "sunbiz_active_percentage": (active_sunbiz / sunbiz_count * 100) if sunbiz_count > 0 else 0
            }

            return health_data

        except Exception as e:
            logger.error(f"Entity data health check failed: {e}")
            return {"error": str(e)}

    async def analyze_data(self) -> Dict[str, Any]:
        """Analyze entity linking patterns"""
        try:
            if not self.entity_ops:
                return {"error": "Entity operations not available"}

            # Find potential duplicates
            duplicates = self.entity_ops.find_potential_duplicates(similarity_threshold=0.85)

            analysis = {
                "potential_duplicates_found": len(duplicates),
                "duplicate_examples": duplicates[:10] if duplicates else [],
                "data_quality_assessment": {
                    "duplicate_rate": len(duplicates) / 1000,  # Approximate rate
                    "needs_cleanup": len(duplicates) > 50
                }
            }

            # Generate alert if many duplicates found
            if len(duplicates) > 100:
                await self.generate_alert(
                    alert_type="DATA_QUALITY",
                    severity="MEDIUM",
                    message=f"High number of potential entity duplicates found: {len(duplicates)}",
                    details={"duplicate_count": len(duplicates), "threshold": 100}
                )

            # Get AI insight
            ai_insight = await self.get_ai_insight(
                analysis,
                "What does the entity duplicate analysis suggest about data quality? What cleanup actions should be prioritized?"
            )
            analysis["ai_insight"] = ai_insight

            return analysis

        except Exception as e:
            logger.error(f"Entity analysis failed: {e}")
            return {"error": str(e)}

class AgentOrchestrator:
    """Orchestrates multiple monitoring agents"""

    def __init__(self, db_manager: DatabaseManager, openai_api_key: str = None):
        self.db_manager = db_manager
        self.openai_api_key = openai_api_key
        self.agents = []
        self.orchestrator_active = False

    def add_agent(self, agent_class, **kwargs):
        """Add an agent to the orchestrator"""
        agent = agent_class(
            db_manager=self.db_manager,
            openai_api_key=self.openai_api_key,
            **kwargs
        )
        self.agents.append(agent)
        logger.info(f"✅ Added agent: {agent.name}")

    async def start_all_agents(self):
        """Start all agents"""
        self.orchestrator_active = True
        logger.info("🚀 Starting all monitoring agents...")

        tasks = []
        for agent in self.agents:
            task = asyncio.create_task(agent.start_monitoring())
            tasks.append(task)

        # Start orchestrator monitoring
        orchestrator_task = asyncio.create_task(self._orchestrator_loop())
        tasks.append(orchestrator_task)

        try:
            await asyncio.gather(*tasks)
        except Exception as e:
            logger.error(f"❌ Agent orchestrator error: {e}")
        finally:
            await self.stop_all_agents()

    async def stop_all_agents(self):
        """Stop all agents"""
        self.orchestrator_active = False
        logger.info("🛑 Stopping all monitoring agents...")

        for agent in self.agents:
            await agent.stop_monitoring()

    async def _orchestrator_loop(self):
        """Main orchestrator monitoring loop"""
        while self.orchestrator_active:
            try:
                # Collect agent statuses
                agent_statuses = [agent.get_status() for agent in self.agents]

                # Check for critical alerts
                critical_alerts = []
                for agent in self.agents:
                    critical_alerts.extend([
                        alert for alert in agent.alerts
                        if alert.severity == "CRITICAL" and not alert.resolved
                    ])

                if critical_alerts:
                    logger.error(f"🚨 {len(critical_alerts)} critical alerts detected")

                # Log orchestrator metrics
                if self.db_manager:
                    monitoring_ops = MonitoringOperations(self.db_manager)
                    monitoring_ops.log_metric(
                        table_name="agent_orchestrator",
                        metric_type="agents_active",
                        metric_value=len([a for a in self.agents if a.status == "ACTIVE"]),
                        metric_unit="count",
                        additional_context={
                            "total_agents": len(self.agents),
                            "critical_alerts": len(critical_alerts)
                        }
                    )

                await asyncio.sleep(60)  # Check every minute

            except Exception as e:
                logger.error(f"Orchestrator loop error: {e}")
                await asyncio.sleep(10)

    def get_system_status(self) -> Dict[str, Any]:
        """Get overall system status"""
        agent_statuses = [asdict(agent.get_status()) for agent in self.agents]

        total_alerts = sum(len([a for a in agent.alerts if not a.resolved]) for agent in self.agents)
        critical_alerts = sum(
            len([a for a in agent.alerts if a.severity == "CRITICAL" and not a.resolved])
            for agent in self.agents
        )

        return {
            "orchestrator_status": "ACTIVE" if self.orchestrator_active else "INACTIVE",
            "agents": agent_statuses,
            "summary": {
                "total_agents": len(self.agents),
                "active_agents": len([a for a in self.agents if a.status == "ACTIVE"]),
                "total_alerts": total_alerts,
                "critical_alerts": critical_alerts
            },
            "timestamp": datetime.now().isoformat()
        }

# Example usage
async def main():
    """Example usage of the monitoring agents"""
    try:
        # Initialize database
        from sqlalchemy_models import initialize_database
        db_manager = initialize_database()

        # Create orchestrator
        orchestrator = AgentOrchestrator(
            db_manager=db_manager,
            openai_api_key=os.getenv('OPENAI_API_KEY')
        )

        # Add agents
        orchestrator.add_agent(PropertyDataAgent, check_interval=300)  # 5 minutes
        orchestrator.add_agent(SalesDataAgent, check_interval=600)     # 10 minutes
        orchestrator.add_agent(TaxCertificateAgent, check_interval=900) # 15 minutes
        orchestrator.add_agent(EntityLinkingAgent, check_interval=1800) # 30 minutes

        # Start monitoring (this will run indefinitely)
        await orchestrator.start_all_agents()

    except KeyboardInterrupt:
        logger.info("🛑 Monitoring stopped by user")
    except Exception as e:
        logger.error(f"❌ Monitoring failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    import os
    asyncio.run(main())